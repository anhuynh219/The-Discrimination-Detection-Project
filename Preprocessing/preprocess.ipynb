{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import underthesea\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Xây_dựng một bộ dữ_liệu hiệu_quả là bước nền_tảng cho nhiều dự_án phân_tích dữ_liệu , học máy và trí_tuệ nhân_tạo . Quá_trình này bao_gồm nhiều bước quan_trọng để đảm_bảo dữ_liệu chất_lượng cao , phù_hợp với mục_tiêu phân_tích và dễ_dàng sử_dụng'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"Xây dựng một bộ dữ liệu hiệu quả là bước nền tảng cho nhiều dự án phân tích dữ liệu, học máy và trí tuệ nhân tạo. Quá trình này bao gồm nhiều bước quan trọng để đảm bảo dữ liệu chất lượng cao, phù hợp với mục tiêu phân tích và dễ dàng sử dụng\"\n",
    "underthesea.word_tokenize(text,format=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py_vncorenlp\n",
    "rdrsegmenter = py_vncorenlp.VnCoreNLP(annotators=[\"wseg\"], save_dir=\"E:\\HK5\\CongNgheDuLieuLon\\DoAn\\VnCoreNLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(txt):\n",
    "    return re.sub(r\"http\\S+\", \"\", txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bang_nguyen_am = [['a', 'à', 'á', 'ả', 'ã', 'ạ', 'a'],\n",
    "                  ['ă', 'ằ', 'ắ', 'ẳ', 'ẵ', 'ặ', 'aw'],\n",
    "                  ['â', 'ầ', 'ấ', 'ẩ', 'ẫ', 'ậ', 'aa'],\n",
    "                  ['e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ', 'e'],\n",
    "                  ['ê', 'ề', 'ế', 'ể', 'ễ', 'ệ', 'ee'],\n",
    "                  ['i', 'ì', 'í', 'ỉ', 'ĩ', 'ị', 'i'],\n",
    "                  ['o', 'ò', 'ó', 'ỏ', 'õ', 'ọ', 'o'],\n",
    "                  ['ô', 'ồ', 'ố', 'ổ', 'ỗ', 'ộ', 'oo'],\n",
    "                  ['ơ', 'ờ', 'ớ', 'ở', 'ỡ', 'ợ', 'ow'],\n",
    "                  ['u', 'ù', 'ú', 'ủ', 'ũ', 'ụ', 'u'],\n",
    "                  ['ư', 'ừ', 'ứ', 'ử', 'ữ', 'ự', 'uw'],\n",
    "                  ['y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ', 'y']]\n",
    "bang_ky_tu_dau = ['', 'f', 's', 'r', 'x', 'j']\n",
    "\n",
    "nguyen_am_to_ids = {}\n",
    "\n",
    "for i in range(len(bang_nguyen_am)):\n",
    "    for j in range(len(bang_nguyen_am[i]) - 1):\n",
    "        nguyen_am_to_ids[bang_nguyen_am[i][j]] = (i, j)\n",
    "\n",
    "\n",
    "def vn_word_to_telex_type(word):\n",
    "    dau_cau = 0\n",
    "    new_word = ''\n",
    "    for char in word:\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x == -1:\n",
    "            new_word += char\n",
    "            continue\n",
    "        if y != 0:\n",
    "            dau_cau = y\n",
    "        new_word += bang_nguyen_am[x][-1]\n",
    "    new_word += bang_ky_tu_dau[dau_cau]\n",
    "    return new_word\n",
    "\n",
    "\n",
    "def vn_sentence_to_telex_type(sentence):\n",
    "    words = sentence.split()\n",
    "    for index, word in enumerate(words):\n",
    "        words[index] = vn_word_to_telex_type(word)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "uniChars = \"àáảãạâầấẩẫậăằắẳẵặèéẻẽẹêềếểễệđìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵÀÁẢÃẠÂẦẤẨẪẬĂẰẮẲẴẶÈÉẺẼẸÊỀẾỂỄỆĐÌÍỈĨỊÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢÙÚỦŨỤƯỪỨỬỮỰỲÝỶỸỴÂĂĐÔƠƯ\"\n",
    "unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n",
    "\n",
    "\n",
    "def loaddicchar():\n",
    "    dic = {}\n",
    "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split(\n",
    "        '|')\n",
    "    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split(\n",
    "        '|')\n",
    "    for i in range(len(char1252)):\n",
    "        dic[char1252[i]] = charutf8[i]\n",
    "    return dic\n",
    "\n",
    "\n",
    "dicchar = loaddicchar()\n",
    "\n",
    "\n",
    "def convert_unicode(txt):\n",
    "    return re.sub(\n",
    "        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
    "        lambda x: dicchar[x.group()], txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chuan_hoa_dau_tu_tieng_viet(word):\n",
    "    if not is_valid_vietnam_word(word):\n",
    "        return word\n",
    "\n",
    "    chars = list(word)\n",
    "    dau_cau = 0\n",
    "    nguyen_am_index = []\n",
    "    qu_or_gi = False\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x == -1:\n",
    "            continue\n",
    "        elif x == 9:  # check qu\n",
    "            if index != 0 and chars[index - 1] == 'q':\n",
    "                chars[index] = 'u'\n",
    "                qu_or_gi = True\n",
    "        elif x == 5:  # check gi\n",
    "            if index != 0 and chars[index - 1] == 'g':\n",
    "                chars[index] = 'i'\n",
    "                qu_or_gi = True\n",
    "        if y != 0:\n",
    "            dau_cau = y\n",
    "            chars[index] = bang_nguyen_am[x][0]\n",
    "        if not qu_or_gi or index != 1:\n",
    "            nguyen_am_index.append(index)\n",
    "    if len(nguyen_am_index) < 2:\n",
    "        if qu_or_gi:\n",
    "            if len(chars) == 2:\n",
    "                x, y = nguyen_am_to_ids.get(chars[1])\n",
    "                chars[1] = bang_nguyen_am[x][dau_cau]\n",
    "            else:\n",
    "                x, y = nguyen_am_to_ids.get(chars[2], (-1, -1))\n",
    "                if x != -1:\n",
    "                    chars[2] = bang_nguyen_am[x][dau_cau]\n",
    "                else:\n",
    "                    chars[1] = bang_nguyen_am[5][dau_cau] if chars[1] == 'i' else bang_nguyen_am[9][dau_cau]\n",
    "            return ''.join(chars)\n",
    "        return word\n",
    "\n",
    "    for index in nguyen_am_index:\n",
    "        x, y = nguyen_am_to_ids[chars[index]]\n",
    "        if x == 4 or x == 8:  # ê, ơ\n",
    "            chars[index] = bang_nguyen_am[x][dau_cau]\n",
    "            # for index2 in nguyen_am_index:\n",
    "            #     if index2 != index:\n",
    "            #         x, y = nguyen_am_to_ids[chars[index]]\n",
    "            #         chars[index2] = bang_nguyen_am[x][0]\n",
    "            return ''.join(chars)\n",
    "\n",
    "    if len(nguyen_am_index) == 2:\n",
    "        if nguyen_am_index[-1] == len(chars) - 1:\n",
    "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "            chars[nguyen_am_index[0]] = bang_nguyen_am[x][dau_cau]\n",
    "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "            # chars[nguyen_am_index[1]] = bang_nguyen_am[x][0]\n",
    "        else:\n",
    "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "            # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
    "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "            chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
    "    else:\n",
    "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "        # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
    "        x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "        chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
    "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[2]]]\n",
    "        # chars[nguyen_am_index[2]] = bang_nguyen_am[x][0]\n",
    "    return ''.join(chars)\n",
    "\n",
    "\n",
    "def is_valid_vietnam_word(word):\n",
    "    chars = list(word)\n",
    "    nguyen_am_index = -1\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x != -1:\n",
    "            if nguyen_am_index == -1:\n",
    "                nguyen_am_index = index\n",
    "            else:\n",
    "                if index - nguyen_am_index != 1:\n",
    "                    return False\n",
    "                nguyen_am_index = index\n",
    "    return True\n",
    "\n",
    "\n",
    "def chuan_hoa_dau_cau_tieng_viet(sentence):\n",
    "    \"\"\"\n",
    "        Chuyển câu tiếng việt về chuẩn gõ dấu kiểu cũ.\n",
    "        :param sentence:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "    sentence = sentence.lower()\n",
    "    words = sentence.split()\n",
    "    for index, word in enumerate(words):\n",
    "        cw = re.sub(r'(^\\p{P}*)([p{L}.]*\\p{L}+)(\\p{P}*$)', r'\\1/\\2/\\3', word).split('/')\n",
    "        # print(cw)\n",
    "        if len(cw) == 3:\n",
    "            cw[1] = chuan_hoa_dau_tu_tieng_viet(cw[1])\n",
    "        words[index] = ''.join(cw)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "teencode_df = pd.read_csv('E:\\HK6\\HMTK_DS102.O21\\[3]-ThucHanh\\Lab04\\Textual data\\Textual data/teencode.txt',names=['teencode','map'],sep='\\t',)\n",
    "teencode_list = teencode_df['teencode'].to_list()\n",
    "map_list = teencode_df['map'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchTeencode(word):\n",
    "  try:\n",
    "    index = teencode_list.index(word)\n",
    "    map_word = map_list[index]\n",
    "    return map_word\n",
    "  except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = 'E:\\HK6\\HMTK_DS102.O21\\[3]-ThucHanh\\Lab04/vietnamese-stopwords.txt'\n",
    "\n",
    "# features extraction\n",
    "with open(STOPWORDS, \"r\",encoding=\"utf-8\") as ins:\n",
    "    stopword = []\n",
    "    for line in ins:\n",
    "        stopword.append(line.strip('\\n'))\n",
    "\n",
    "def remove_stopwords(line):\n",
    "    words = []\n",
    "    for word in line:\n",
    "        if word not in stopword:\n",
    "            words.append(word)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant\n",
    "eng = enchant.Dict(\"en_US\")\n",
    "def remove_dub_char(sentence):\n",
    "  sentence = str(sentence)\n",
    "  words = []\n",
    "  for word in sentence.strip().split():\n",
    "    if word in teencode_list:\n",
    "      words.append(word)\n",
    "      continue\n",
    "    if eng.check(str(word)):\n",
    "      words.append(word)\n",
    "      continue\n",
    "    words.append(re.sub(r'([A-Z])\\1+', lambda m: m.group(1), word, flags = re.IGNORECASE))\n",
    "  return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def teencode(sentence):\n",
    "  sentence = str(sentence)\n",
    "  #Tokenize\n",
    "  List_tokens = word_tokenize(sentence)\n",
    "  #Teencode\n",
    "  for tokens_idx, text_tokens in enumerate(List_tokens):\n",
    "    deteencoded = searchTeencode(text_tokens)\n",
    "    if (deteencoded != None):\n",
    "        List_tokens[tokens_idx] = deteencoded\n",
    "  text=TreebankWordDetokenizer().detokenize(List_tokens)\n",
    "  return text\n",
    "def stopwords(sentence):\n",
    "  sentence = str(sentence)\n",
    "  #Tokenize\n",
    "  List_tokens = word_tokenize(sentence)\n",
    "  #Teencode\n",
    "  for tokens_idx, text_tokens in enumerate(List_tokens):\n",
    "    List_tokens[tokens_idx] = text_tokens\n",
    "\n",
    "  #Stopwords\n",
    "  list_without_sw = remove_stopwords(List_tokens)\n",
    "\n",
    "  return TreebankWordDetokenizer().detokenize(list_without_sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text:str,stopword_flag:bool=True):\n",
    "    if not text:\n",
    "        return text\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = teencode(text)\n",
    "    text = rdrsegmenter.word_segment(text)\n",
    "    text = ''.join(text)\n",
    "    text = re.sub(r'[@,#]\\w+\\b', '', text)\n",
    "    special_chars_pattern = r\"[^\\w\\s]\"\n",
    "    text = re.sub(special_chars_pattern, '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = remove_dub_char(text)\n",
    "    text = remove_html(text)\n",
    "    text = convert_unicode(text)\n",
    "    text = chuan_hoa_dau_cau_tieng_viet(text)\n",
    "    if stopword_flag:\n",
    "        text = stopwords(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def check_file_type(file_path):\n",
    "    \"\"\"\n",
    "    Check if a file exists and determine its type (CSV, XLSX, XML, Pickle, JSON, or HTML).\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing (file_exists, file_type).\n",
    "        - file_exists (bool): True if the file exists, False otherwise.\n",
    "        - file_type (str): The file type if the file exists, or an empty string if the file doesn't exist or the type cannot be determined.\n",
    "    \"\"\"\n",
    "\n",
    "    file_exists = os.path.exists(file_path)\n",
    "    if not file_exists:\n",
    "        return file_exists, \"\"\n",
    "\n",
    "    file_type = \"\"\n",
    "    filename, extension = os.path.splitext(file_path)\n",
    "\n",
    "    if extension.lower() in ['.csv']:\n",
    "        file_type = \"CSV\"\n",
    "    elif extension.lower() in ['.xlsx']:\n",
    "        file_type = \"XLSX\"\n",
    "    elif extension.lower() in ['.xml']:\n",
    "        file_type = \"XML\"\n",
    "    elif extension.lower() in ['.pkl', '.pickle']:\n",
    "        file_type = \"Pickle\"\n",
    "    elif extension.lower() in ['.json']:\n",
    "        file_type = \"JSON\"\n",
    "    elif extension.lower() in ['.html', '.htm']:\n",
    "        file_type = \"HTML\"\n",
    "\n",
    "    return file_exists, file_type\n",
    "def read_file_to_df(file_path):\n",
    "    \"\"\"\n",
    "    Read a file into a Pandas DataFrame based on its file type.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the file.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The DataFrame containing the data from the file.\n",
    "        None if the file does not exist or the file type is not supported.\n",
    "    \"\"\"\n",
    "\n",
    "    file_exists, file_type = check_file_type(file_path)\n",
    "    if not file_exists:\n",
    "        return None\n",
    "\n",
    "    if file_type == \"CSV\":\n",
    "        read_func = pd.read_csv\n",
    "    elif file_type == \"XLSX\":\n",
    "        read_func = pd.read_excel\n",
    "    elif file_type == \"JSON\":\n",
    "        read_func = pd.read_json\n",
    "    elif file_type == \"Pickle\":\n",
    "        read_func = pd.read_pickle\n",
    "    elif file_type == \"HTML\":\n",
    "        read_func = pd.read_html\n",
    "    elif file_type == \"XML\":\n",
    "        read_func = pd.read_xml\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_type}\")\n",
    "\n",
    "    try:\n",
    "        df = read_func(file_path)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error reading file {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_pipe(input: str|pd.DataFrame,col_name:str,stopword_flag:bool=False,store_flag:bool=False):\n",
    "    if type(input)==str:\n",
    "        df=read_file_to_df(input)\n",
    "        df[\"processed_\"+col_name]=df[col_name].apply(lambda x: preprocessing(x,stopword_flag=stopword_flag))\n",
    "        if store_flag:\n",
    "            df.to_csv(input,index=False)\n",
    "    elif type(input)==pd.DataFrame:\n",
    "        df=input\n",
    "        df[\"processed_\"+col_name]=df[col_name].apply(lambda x: preprocessing(x,stopword_flag=stopword_flag))\n",
    "        if store_flag:\n",
    "            df.to_csv(input,index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1582 entries, 0 to 1581\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   index           1582 non-null   int64  \n",
      " 1   author          1581 non-null   object \n",
      " 2   updated_at      1582 non-null   object \n",
      " 3   like_count      1582 non-null   int64  \n",
      " 4   text            1582 non-null   object \n",
      " 5   video_id        1582 non-null   object \n",
      " 6   public          1582 non-null   bool   \n",
      " 7   label           1582 non-null   float64\n",
      " 8   processed_text  1527 non-null   object \n",
      " 9   platform        1582 non-null   object \n",
      "dtypes: bool(1), float64(1), int64(2), object(6)\n",
      "memory usage: 112.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(r\"E:\\HK6\\HMTK_DS102.O21\\[3]-ThucHanh\\Lab04\\data\\yt_short.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_8472\\2804732716.py:1: FutureWarning: Series.replace without 'value' and with non-dict-like 'to_replace' is deprecated and will raise in a future version. Explicitly specify the new values instead.\n",
      "  df['label'].replace({'  ',0.0},inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df['label'].replace({'  ',0.0},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>author</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>like_count</th>\n",
       "      <th>text</th>\n",
       "      <th>video_id</th>\n",
       "      <th>public</th>\n",
       "      <th>label</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>platform</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>133</td>\n",
       "      <td>@hoangngoc3518</td>\n",
       "      <td>2024-01-02T07:30:45Z</td>\n",
       "      <td>0</td>\n",
       "      <td>Tư tưởng học đòi giàu sang hưởng thụ sẽ dẫn đế...</td>\n",
       "      <td>cEvCPS4eU40</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>tư_tưởng học_đòi giàu_sang hưởng_thụ sẽ dẫn đế...</td>\n",
       "      <td>Youtube Short</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index          author            updated_at  like_count  \\\n",
       "1504    133  @hoangngoc3518  2024-01-02T07:30:45Z           0   \n",
       "\n",
       "                                                   text     video_id  public  \\\n",
       "1504  Tư tưởng học đòi giàu sang hưởng thụ sẽ dẫn đế...  cEvCPS4eU40    True   \n",
       "\n",
       "     label                                     processed_text       platform  \n",
       "1504        tư_tưởng học_đòi giàu_sang hưởng_thụ sẽ dẫn đế...  Youtube Short  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['label']==' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5       hồi nhỏ sống ở 1 nơi toàn những người thích hồ...\n",
       "19                        bắc kì nam kì trung kì đồng_bào\n",
       "26      parky có nghĩa là việt cộng nhé mọi người ở vi...\n",
       "30      mình nhớ hồi học_sinh cấp 1 cho đến cho cấp 3 ...\n",
       "33       chúng_ta phải nên tự_hào vì chúng_ta là bắc kỳ k\n",
       "                              ...                        \n",
       "1222                            sao lại là parky và namky\n",
       "1388    mình quê thanh_hoá có bị phân_biệt không cứ kh...\n",
       "1549    bản_chất ăn_cháo_đá_bát vong ơn phụ nghĩa chối...\n",
       "1557    tôi thích người ta gọi mình là nhà_quê khi đó ...\n",
       "1559    người nhà quê thường có ý trí phấn_đấu cao hơn...\n",
       "Name: processed_text, Length: 86, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df['processed_text'][df['label']==4.0],clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1582 entries, 0 to 1581\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   index           1582 non-null   int64 \n",
      " 1   author          1581 non-null   object\n",
      " 2   updated_at      1582 non-null   object\n",
      " 3   like_count      1582 non-null   int64 \n",
      " 4   text            1582 non-null   object\n",
      " 5   video_id        1582 non-null   object\n",
      " 6   public          1582 non-null   bool  \n",
      " 7   label           1582 non-null   object\n",
      " 8   processed_text  1527 non-null   object\n",
      " 9   platform        1582 non-null   object\n",
      "dtypes: bool(1), int64(2), object(7)\n",
      "memory usage: 112.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8417 entries, 0 to 8416\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   index           8417 non-null   int64  \n",
      " 1   author          8386 non-null   object \n",
      " 2   updated_at      8417 non-null   object \n",
      " 3   like_count      8417 non-null   int64  \n",
      " 4   text            8414 non-null   object \n",
      " 5   video_id        8417 non-null   object \n",
      " 6   public          8417 non-null   bool   \n",
      " 7   label           8417 non-null   float64\n",
      " 8   processed_text  8417 non-null   object \n",
      " 9   platform        8417 non-null   object \n",
      "dtypes: bool(1), float64(1), int64(2), object(6)\n",
      "memory usage: 600.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['platform']=\"Youtube\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename({\"Unnamed: 0\":\"index\"},axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename({\"comment\":\"text\"},axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].replace({4:4.0,3:3.0,2:2.0,1:1.0,np.nan:0.0,},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=preprocessing_pipe(df,col_name=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>platform</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>namki đồng_nai chào anh_em parky</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TikTok</td>\n",
       "      <td>namki đồng_nai chào anh_em parky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>parky</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TikTok</td>\n",
       "      <td>parky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>nam bắc</td>\n",
       "      <td>2.0</td>\n",
       "      <td>TikTok</td>\n",
       "      <td>nam bắc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>parky</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TikTok</td>\n",
       "      <td>parky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>chung miền nam không parky</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TikTok</td>\n",
       "      <td>chung miền nam không parky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>miền bắc parky</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TikTok</td>\n",
       "      <td>miền bắc parky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>hào parky</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TikTok</td>\n",
       "      <td>hào parky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>kid parky</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TikTok</td>\n",
       "      <td>kid parky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>parky</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TikTok</td>\n",
       "      <td>parky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>miền nam tiếc bình_luận tiêu_cực vầy thành lỗi...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>TikTok</td>\n",
       "      <td>miền nam tiếc bình_luận tiêu_cực vầy thành lỗi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               text  label platform  \\\n",
       "0      0                   namki đồng_nai chào anh_em parky    1.0   TikTok   \n",
       "1      1                                              parky    1.0   TikTok   \n",
       "2      2                                            nam bắc    2.0   TikTok   \n",
       "3      3                                              parky    1.0   TikTok   \n",
       "4      4                         chung miền nam không parky    1.0   TikTok   \n",
       "5      5                                     miền bắc parky    1.0   TikTok   \n",
       "6      6                                          hào parky    1.0   TikTok   \n",
       "7      7                                          kid parky    1.0   TikTok   \n",
       "8      8                                              parky    1.0   TikTok   \n",
       "9      9  miền nam tiếc bình_luận tiêu_cực vầy thành lỗi...    2.0   TikTok   \n",
       "\n",
       "                                      processed_text  \n",
       "0                   namki đồng_nai chào anh_em parky  \n",
       "1                                              parky  \n",
       "2                                            nam bắc  \n",
       "3                                              parky  \n",
       "4                         chung miền nam không parky  \n",
       "5                                     miền bắc parky  \n",
       "6                                          hào parky  \n",
       "7                                          kid parky  \n",
       "8                                              parky  \n",
       "9  miền nam tiếc bình_luận tiêu_cực vầy thành lỗi...  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r\"E:\\HK6\\HMTK_DS102.O21\\[3]-ThucHanh\\Lab04\\data\\tiktok.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
